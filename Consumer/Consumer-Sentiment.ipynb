{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Initialize NLTK's Vader and download stopwords\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = df.select(\n",
    "                explode(split(col(\"filtered_title\"), \" \")).alias(\"word\")\n",
    "            ).filter(\n",
    "                (col(\"word\").like(\"/u/%\")) |\n",
    "                (col(\"word\").like(\"/r/%\")) |\n",
    "                (col(\"word\").like(\"http%\"))\n",
    "            )\n",
    "            print(\"-----------------------------\")\n",
    "            print(references.collect())\n",
    "            print(\"-----------------------------\")\n",
    "            # User references\n",
    "            user_refs = references.filter(col(\"word\").like(\"/u/%\")).groupBy(\"word\").count().withColumnRenamed(\"count\", \"user_references_count\")\n",
    "            user_refs.createOrReplaceTempView(\"user_references\")\n",
    "            user_refs.write.json(\"/data/output/user_references\", mode=\"append\")\n",
    "            print(\"User references table created and saved.\")\n",
    "\n",
    "            # Post references\n",
    "            post_refs = references.filter(col(\"word\").like(\"/r/%\")).groupBy(\"word\").count().withColumnRenamed(\"count\", \"post_references_count\")\n",
    "            post_refs.createOrReplaceTempView(\"post_references\")\n",
    "            post_refs.write.json(\"/data/output/post_references\", mode=\"append\")\n",
    "            print(\"Post references table created and saved.\")\n",
    "            \n",
    "            # URL references\n",
    "            url_refs = references.filter(col(\"word\").like(\"http%\")).groupBy(\"word\").count().withColumnRenamed(\"count\", \"url_references_count\")\n",
    "            url_refs.createOrReplaceTempView(\"url_references\")\n",
    "            url_refs.write.json(\"/data/output/url_references\", mode=\"append\")\n",
    "            print(\"URL references table created and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/09 15:34:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/opt/bitnami/spark/python/pyspark/streaming/context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/09 15:34:51 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "24/06/09 15:34:54 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "24/06/09 15:34:56 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "24/06/09 15:34:59 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing RDD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/09 15:35:02 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "24/06/09 15:35:05 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+---------+----------------------------------------------------------------------+--------------------------------------------+--------+---------+\n",
      "|author     |date               |subreddit|title                                                                 |filtered_title                              |polarity|sentiment|\n",
      "+-----------+-------------------+---------+----------------------------------------------------------------------+--------------------------------------------+--------+---------+\n",
      "|usrnmthisis|2024-06-09 15:08:13|AskReddit|why do you think cats would rather starve than eating what they hunt ?|think cats would rather starve eating hunt ?|-0.4404 |negative |\n",
      "|usrnmthisis|2024-06-09 15:08:13|AskReddit|why do you think cats would rather starve than eating what they hunt ?|think cats would rather starve eating hunt ?|-0.4404 |negative |\n",
      "|usrnmthisis|2024-06-09 15:08:13|AskReddit|why do you think cats would rather starve than eating what they hunt ?|think cats would rather starve eating hunt ?|-0.4404 |negative |\n",
      "+-----------+-------------------+---------+----------------------------------------------------------------------+--------------------------------------------+--------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/09 15:35:06 ERROR JobScheduler: Error running job streaming job 1717947300000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/bitnami/spark/python/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/bitnami/spark/python/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_38846/3316594128.py\", line 158, in <lambda>\n",
      "    lines.foreachRDD(lambda rdd: process_rdd(rdd))\n",
      "                                 ^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_38846/3316594128.py\", line 123, in process_rdd\n",
      "    tfidf_data = tf_idf(df)\n",
      "                 ^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_38846/3316594128.py\", line 66, in tf_idf\n",
      "    tf = hashing_tf.transform(words_data)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/bitnami/spark/python/pyspark/ml/base.py\", line 262, in transform\n",
      "    return self._transform(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/bitnami/spark/python/pyspark/ml/wrapper.py\", line 398, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sparkSession)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/bitnami/python/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/bitnami/spark/python/pyspark/errors/exceptions/captured.py\", line 185, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.IllegalArgumentException: requirement failed: The input column must be array, but got string.\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o30.awaitTermination.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"/opt/bitnami/spark/python/pyspark/streaming/util.py\", line 71, in call\n    r = self.func(t, *rdds)\n        ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/spark/python/pyspark/streaming/dstream.py\", line 236, in func\n    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n           ^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_38846/3316594128.py\", line 158, in <lambda>\n    lines.foreachRDD(lambda rdd: process_rdd(rdd))\n                                 ^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_38846/3316594128.py\", line 123, in process_rdd\n    tfidf_data = tf_idf(df)\n                 ^^^^^^^^^^\n  File \"/tmp/ipykernel_38846/3316594128.py\", line 66, in tf_idf\n    tf = hashing_tf.transform(words_data)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/spark/python/pyspark/ml/base.py\", line 262, in transform\n    return self._transform(dataset)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/spark/python/pyspark/ml/wrapper.py\", line 398, in _transform\n    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sparkSession)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/python/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/spark/python/pyspark/errors/exceptions/captured.py\", line 185, in deco\n    raise converted from None\npyspark.errors.exceptions.captured.IllegalArgumentException: requirement failed: The input column must be array, but got string.\n\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 166\u001b[0m\n\u001b[1;32m    164\u001b[0m ssc\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStreaming started...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 166\u001b[0m \u001b[43mssc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/streaming/context.py:239\u001b[0m, in \u001b[0;36mStreamingContext.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03mWait for the execution to stop.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m    time to wait in seconds\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jssc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jssc\u001b[38;5;241m.\u001b[39mawaitTerminationOrTimeout(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o30.awaitTermination.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"/opt/bitnami/spark/python/pyspark/streaming/util.py\", line 71, in call\n    r = self.func(t, *rdds)\n        ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/spark/python/pyspark/streaming/dstream.py\", line 236, in func\n    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n           ^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_38846/3316594128.py\", line 158, in <lambda>\n    lines.foreachRDD(lambda rdd: process_rdd(rdd))\n                                 ^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_38846/3316594128.py\", line 123, in process_rdd\n    tfidf_data = tf_idf(df)\n                 ^^^^^^^^^^\n  File \"/tmp/ipykernel_38846/3316594128.py\", line 66, in tf_idf\n    tf = hashing_tf.transform(words_data)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/spark/python/pyspark/ml/base.py\", line 262, in transform\n    return self._transform(dataset)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/spark/python/pyspark/ml/wrapper.py\", line 398, in _transform\n    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sparkSession)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/python/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/spark/python/pyspark/errors/exceptions/captured.py\", line 185, in deco\n    raise converted from None\npyspark.errors.exceptions.captured.IllegalArgumentException: requirement failed: The input column must be array, but got string.\n\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing RDD...\n",
      "+-----------+-------------------+---------+----------------------------------------------------------------------+--------------------------------------------+--------+---------+\n",
      "|author     |date               |subreddit|title                                                                 |filtered_title                              |polarity|sentiment|\n",
      "+-----------+-------------------+---------+----------------------------------------------------------------------+--------------------------------------------+--------+---------+\n",
      "|usrnmthisis|2024-06-09 15:08:13|AskReddit|why do you think cats would rather starve than eating what they hunt ?|think cats would rather starve eating hunt ?|-0.4404 |negative |\n",
      "|usrnmthisis|2024-06-09 15:08:13|AskReddit|why do you think cats would rather starve than eating what they hunt ?|think cats would rather starve eating hunt ?|-0.4404 |negative |\n",
      "|usrnmthisis|2024-06-09 15:08:13|AskReddit|why do you think cats would rather starve than eating what they hunt ?|think cats would rather starve eating hunt ?|-0.4404 |negative |\n",
      "|usrnmthisis|2024-06-09 15:08:13|AskReddit|why do you think cats would rather starve than eating what they hunt ?|think cats would rather starve eating hunt ?|-0.4404 |negative |\n",
      "|usrnmthisis|2024-06-09 15:08:13|AskReddit|why do you think cats would rather starve than eating what they hunt ?|think cats would rather starve eating hunt ?|-0.4404 |negative |\n",
      "+-----------+-------------------+---------+----------------------------------------------------------------------+--------------------------------------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing RDD...\n",
      "+-------------------+-------------------+---------+----------------------------------------------------------------+---------------------------------------+--------+---------+\n",
      "|author             |date               |subreddit|title                                                           |filtered_title                         |polarity|sentiment|\n",
      "+-------------------+-------------------+---------+----------------------------------------------------------------+---------------------------------------+--------+---------+\n",
      "|Ok_Marionberry_3253|2024-06-09 15:08:58|AskReddit|If you could change one thing about the world, what would it be?|could change one thing world, would be?|0.0     |neutral  |\n",
      "|Ok_Marionberry_3253|2024-06-09 15:08:58|AskReddit|If you could change one thing about the world, what would it be?|could change one thing world, would be?|0.0     |neutral  |\n",
      "|Ok_Marionberry_3253|2024-06-09 15:08:58|AskReddit|If you could change one thing about the world, what would it be?|could change one thing world, would be?|0.0     |neutral  |\n",
      "|Ok_Marionberry_3253|2024-06-09 15:08:58|AskReddit|If you could change one thing about the world, what would it be?|could change one thing world, would be?|0.0     |neutral  |\n",
      "|Ok_Marionberry_3253|2024-06-09 15:08:58|AskReddit|If you could change one thing about the world, what would it be?|could change one thing world, would be?|0.0     |neutral  |\n",
      "+-------------------+-------------------+---------+----------------------------------------------------------------+---------------------------------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing RDD...\n",
      "+-------------------+-------------------+---------+-------------------------------------------------------------------------------------+------------------------------------------------------+--------+---------+\n",
      "|author             |date               |subreddit|title                                                                                |filtered_title                                        |polarity|sentiment|\n",
      "+-------------------+-------------------+---------+-------------------------------------------------------------------------------------+------------------------------------------------------+--------+---------+\n",
      "|ManaHijinx         |2024-06-09 15:09:20|AskReddit|What song would you pick to be the sound track to all of your life’s biggest moments?|song would pick sound track life’s biggest moments?   |0.0     |neutral  |\n",
      "|ManaHijinx         |2024-06-09 15:09:20|AskReddit|What song would you pick to be the sound track to all of your life’s biggest moments?|song would pick sound track life’s biggest moments?   |0.0     |neutral  |\n",
      "|Ok_Marionberry_3253|2024-06-09 15:09:36|AskReddit|What’s a random fact you know that others might find interesting?                    |What’s random fact know others might find interesting?|0.4019  |positive |\n",
      "|Ok_Marionberry_3253|2024-06-09 15:09:36|AskReddit|What’s a random fact you know that others might find interesting?                    |What’s random fact know others might find interesting?|0.4019  |positive |\n",
      "|Ok_Marionberry_3253|2024-06-09 15:09:36|AskReddit|What’s a random fact you know that others might find interesting?                    |What’s random fact know others might find interesting?|0.4019  |positive |\n",
      "+-------------------+-------------------+---------+-------------------------------------------------------------------------------------+------------------------------------------------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing RDD...\n",
      "+-------------------+-------------------+---------+-----------------------------------------------------------------+------------------------------------------------------+--------+---------+\n",
      "|author             |date               |subreddit|title                                                            |filtered_title                                        |polarity|sentiment|\n",
      "+-------------------+-------------------+---------+-----------------------------------------------------------------+------------------------------------------------------+--------+---------+\n",
      "|Ok_Marionberry_3253|2024-06-09 15:09:18|AskReddit|What's the funniest thing that's happened to you recently?       |What's funniest thing that's happened recently?       |0.5574  |positive |\n",
      "|Ok_Marionberry_3253|2024-06-09 15:09:36|AskReddit|What’s a random fact you know that others might find interesting?|What’s random fact know others might find interesting?|0.4019  |positive |\n",
      "|Ok_Marionberry_3253|2024-06-09 15:09:36|AskReddit|What’s a random fact you know that others might find interesting?|What’s random fact know others might find interesting?|0.4019  |positive |\n",
      "|Ok_Marionberry_3253|2024-06-09 15:09:36|AskReddit|What’s a random fact you know that others might find interesting?|What’s random fact know others might find interesting?|0.4019  |positive |\n",
      "|Ok_Marionberry_3253|2024-06-09 15:09:36|AskReddit|What’s a random fact you know that others might find interesting?|What’s random fact know others might find interesting?|0.4019  |positive |\n",
      "+-------------------+-------------------+---------+-----------------------------------------------------------------+------------------------------------------------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing RDD...\n",
      "+--------------------+-------------------+---------+-----------------------------------------------------------------+------------------------------------------------------+--------+---------+\n",
      "|author              |date               |subreddit|title                                                            |filtered_title                                        |polarity|sentiment|\n",
      "+--------------------+-------------------+---------+-----------------------------------------------------------------+------------------------------------------------------+--------+---------+\n",
      "|Ok_Marionberry_3253 |2024-06-09 15:09:36|AskReddit|What’s a random fact you know that others might find interesting?|What’s random fact know others might find interesting?|0.4019  |positive |\n",
      "|Ok_Marionberry_3253 |2024-06-09 15:09:36|AskReddit|What’s a random fact you know that others might find interesting?|What’s random fact know others might find interesting?|0.4019  |positive |\n",
      "|Ok_Marionberry_3253 |2024-06-09 15:09:36|AskReddit|What’s a random fact you know that others might find interesting?|What’s random fact know others might find interesting?|0.4019  |positive |\n",
      "|Affectionate-Cap-636|2024-06-09 15:09:42|AskReddit|Bra straps views lovers… who likes it?                           |Bra straps views lovers… likes it?                    |0.4215  |positive |\n",
      "|Affectionate-Cap-636|2024-06-09 15:09:42|AskReddit|Bra straps views lovers… who likes it?                           |Bra straps views lovers… likes it?                    |0.4215  |positive |\n",
      "+--------------------+-------------------+---------+-----------------------------------------------------------------+------------------------------------------------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing RDD...\n",
      "+--------------------+-------------------+---------+--------------------------------------+----------------------------------+--------+---------+\n",
      "|author              |date               |subreddit|title                                 |filtered_title                    |polarity|sentiment|\n",
      "+--------------------+-------------------+---------+--------------------------------------+----------------------------------+--------+---------+\n",
      "|Affectionate-Cap-636|2024-06-09 15:09:42|AskReddit|Bra straps views lovers… who likes it?|Bra straps views lovers… likes it?|0.4215  |positive |\n",
      "|Affectionate-Cap-636|2024-06-09 15:09:42|AskReddit|Bra straps views lovers… who likes it?|Bra straps views lovers… likes it?|0.4215  |positive |\n",
      "|Affectionate-Cap-636|2024-06-09 15:09:42|AskReddit|Bra straps views lovers… who likes it?|Bra straps views lovers… likes it?|0.4215  |positive |\n",
      "|Affectionate-Cap-636|2024-06-09 15:09:42|AskReddit|Bra straps views lovers… who likes it?|Bra straps views lovers… likes it?|0.4215  |positive |\n",
      "|Affectionate-Cap-636|2024-06-09 15:09:42|AskReddit|Bra straps views lovers… who likes it?|Bra straps views lovers… likes it?|0.4215  |positive |\n",
      "+--------------------+-------------------+---------+--------------------------------------+----------------------------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import explode, split, col, udf, min, max, lit\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import os\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define the host and port\n",
    "host = \"127.0.0.1\"\n",
    "port = 9998\n",
    "\n",
    "# Initialize SparkContext and SparkSession\n",
    "sc = SparkContext(\"local[2]\", \"RedditConsumer\")\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# Initialize StreamingContext with a batch interval of 10 seconds\n",
    "ssc = StreamingContext(sc, 60)\n",
    "\n",
    "# Ensure the necessary directories exist\n",
    "os.makedirs(\"/data/raw/reddit_ask\", exist_ok=True)\n",
    "os.makedirs(\"/data/output/user_references\", exist_ok=True)\n",
    "os.makedirs(\"/data/output/post_references\", exist_ok=True)\n",
    "os.makedirs(\"/data/output/url_references\", exist_ok=True)\n",
    "os.makedirs(\"/data/output/top_words\", exist_ok=True)\n",
    "os.makedirs(\"/data/output/metrics\", exist_ok=True)\n",
    "os.makedirs(\"/data/output/sentiment_analysis\", exist_ok=True)\n",
    "\n",
    "def get_sentiment(text):\n",
    "    scores = sid.polarity_scores(text)\n",
    "    return scores['compound']\n",
    "\n",
    "def classify_sentiment(polarity):\n",
    "    if polarity > 0.05:\n",
    "        return 'positive'\n",
    "    elif polarity < -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "get_sentiment_udf = udf(get_sentiment, DoubleType())\n",
    "classify_sentiment_udf = udf(classify_sentiment, StringType())\n",
    "remove_stopwords_udf = udf(remove_stopwords, StringType())\n",
    "def new_columns(df):\n",
    "    # Remove stopwords from titles\n",
    "    df = df.withColumn(\"filtered_title\", remove_stopwords_udf(col(\"title\")))\n",
    "    df = df.withColumn(\"polarity\", get_sentiment_udf(col(\"filtered_title\")))\n",
    "    df = df.withColumn(\"sentiment\", classify_sentiment_udf(col(\"polarity\")))\n",
    "    return df\n",
    "def tf_idf(df):\n",
    "    words_data = df.select(explode(split(col(\"filtered_title\"), \" \")).alias(\"words\"))\n",
    "    hashing_tf = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "    tf = hashing_tf.transform(words_data)\n",
    "    idf_model = idf.fit(tf)\n",
    "    return idf_model.transform(tf)\n",
    "def time_optional(df):\n",
    "    # Get the time range of the data\n",
    "    time_range = df.select(min(\"date\").alias(\"start_time\"), max(\"date\").alias(\"end_time\")).collect()\n",
    "    if time_range:\n",
    "        start_time = time_range[0][\"start_time\"]\n",
    "        end_time = time_range[0][\"end_time\"]\n",
    "        print(f\"Time range: {start_time} to {end_time}\")\n",
    "def metric(user_refs):\n",
    "    #optional\n",
    "        # Store metrics in a temporary table\n",
    "    metrics_df = user_refs.join(post_refs, \"word\", \"outer\") \\\n",
    "                          .join(url_refs, \"word\", \"outer\")\n",
    "    metrics_df = metrics_df.withColumn(\"average_sentiment\", lit(average_sentiment))\n",
    "    metrics_df = metrics_df.withColumn(\"start_time\", lit(start_time))\n",
    "    metrics_df = metrics_df.withColumn(\"end_time\", lit(end_time))\n",
    "    metrics_df.createOrReplaceTempView(\"metrics\")\n",
    "    metrics_df.write.json(\"/data/output/metrics\", mode=\"append\")\n",
    "    print(\"Metrics table created and saved.\")\n",
    "    metrics_df.show(5, truncate=False)\n",
    "\n",
    "def placeholder()\n",
    "    # Calculate TF-IDF\n",
    "            \n",
    "    tfidf_data = tf_idf(df)\n",
    "    print(tfidf_data)\n",
    "    # Find the top words and their TF-IDF scores\n",
    "    top_words = tfidf_data.select(\"words\", \"features\").rdd.flatMap(\n",
    "        lambda row: [(row.words, float(v)) for v in row.features.toArray()]\n",
    "    ).sortBy(lambda x: -x[1]).take(10)\n",
    "    print(top_words.collect())\n",
    "    # Convert top words and their TF-IDF scores to DataFrame and persist it\n",
    "    top_words_df = spark.createDataFrame(top_words, [\"word\", \"tfidf\"])\n",
    "    top_words_df.createOrReplaceTempView(\"top_words\")\n",
    "    #top_words_df.write.json(\"/data/output/top_words\", mode=\"append\")\n",
    "    print(spark.sql(\"select * from top_words\"))\n",
    "    print(\"TF-IDF table created and saved.\")\n",
    "    \n",
    "    # Sentiment Analysis using NLTK's Vader\n",
    "    sentiment_df = df.select(\"title\", \"polarity\", \"sentiment\")\n",
    "    sentiment_df.createOrReplaceTempView(\"sentiment_analysis\")\n",
    "    sentiment_df.write.json(\"/data/output/sentiment_analysis\", mode=\"append\")\n",
    "    print(\"Sentiment analysis table created and saved.\")\n",
    "    \n",
    "    sentiment_rdd = df.select(\"polarity\").rdd.map(lambda row: row.polarity)\n",
    "    average_sentiment = sentiment_rdd.mean()\n",
    "    print(f\"Average sentiment: {average_sentiment}\")\n",
    "\n",
    "def process_rdd(rdd):\n",
    "    \"\"\"\n",
    "    Process each RDD:\n",
    "    1. Convert RDD to DataFrame and persist it.\n",
    "    2. Extract references to users, posts, and URLs.\n",
    "    3. Calculate TF-IDF.\n",
    "    4. Find the top words and their TF-IDF scores.\n",
    "    5. Perform sentiment analysis.\n",
    "    6. Store metrics in a temporary table.\n",
    "    7. Get the time range of the data.\n",
    "    \"\"\"\n",
    "    print(\"Processing RDD...\")\n",
    "    if not rdd.isEmpty():\n",
    "        # Parse each record as JSON\n",
    "        records = rdd.map(lambda record: json.loads(record)).collect()\n",
    "        \n",
    "        # Check if records are valid and not empty\n",
    "        if records:\n",
    "            # Convert RDD to DataFrame\n",
    "            df = spark.createDataFrame(records)\n",
    "            df = new_columns(df)\n",
    "          \n",
    "            # Save raw data\n",
    "            df.createOrReplaceTempView(\"raw\")\n",
    "            df.write.json(\"/data/raw/reddit_ask\", mode=\"append\")\n",
    "            \n",
    "            # Print DataFrame schema and some data\n",
    "            #df.printSchema()\n",
    "            df.show(5, truncate=False)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "        else:\n",
    "            print(\"No valid records found in this RDD.\")\n",
    "\n",
    "# Define the socket stream\n",
    "lines = ssc.socketTextStream(host, port)\n",
    "\n",
    "# Apply the processing function to each RDD in the DStream\n",
    "lines.foreachRDD(lambda rdd: process_rdd(rdd))\n",
    "\n",
    "# Print received lines (for debugging)\n",
    "#lines.pprint()\n",
    "\n",
    "# Start the streaming context and wait for termination\n",
    "ssc.start()\n",
    "print(\"Streaming started...\")\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

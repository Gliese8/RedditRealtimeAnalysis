{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aca7ae20-a247-4ee1-953a-1376a58c63ee",
   "metadata": {},
   "source": [
    "### Consumer\n",
    "\n",
    "This consumer should recieve the reddit data from the producer and perform data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05d2938-5b27-4585-919d-b9e6711abf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the SparkContext\n",
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a28ba35-ca67-4451-b5e0-104a5af2e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cdee61-1750-4162-939e-f31b9bb4e50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c07b969-0c08-4166-9739-bbb43b6ccc16",
   "metadata": {},
   "source": [
    "- Take the raw Producer Data and save it to files on disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959e4e35-ae3e-478e-ad74-be9e939fab96",
   "metadata": {},
   "source": [
    "- Get the references and the number of occurrences (suggested in 60 seconds windows every 5 seconds). Note: You have to let it run for at least 60 seconds and then each window is sliding (5 seconds). You will see the lines count for each window then fluctuates between 19 and 22 as it drops the previous posts and aggregates the new posts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1beea9f-62ea-40e9-b12c-422ed05749ca",
   "metadata": {},
   "source": [
    "##### !!!  Need to add the \"Get top 10 important words in window using TF-IDF\"  !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6fa061-d425-43c9-95a2-c7adec0c720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Create a SparkSession and StreamingContext\n",
    "spark_conf = SparkConf().setAppName(\"reddit\")\n",
    "ss1 = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "ssc = StreamingContext(ss1.sparkContext, 5)\n",
    "\n",
    "# Create a DStream\n",
    "lines = ssc.socketTextStream(\"localhost\", 9998)\n",
    "\n",
    "# Apply windowing to the DStream\n",
    "windowed_lines = lines.window(60, 5)\n",
    "\n",
    "# Parse the JSON data\n",
    "comments = windowed_lines.map(lambda json_data: json.loads(json_data))\n",
    "comments.pprint()\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"comment\", StringType(), True),\n",
    "    StructField(\"prev_comment\", StringType(), True),\n",
    "    StructField(\"post\", StringType(), True),\n",
    "    StructField(\"author\", StringType(), True),\n",
    "    StructField(\"created_utc\", StringType(), True),\n",
    "    StructField(\"link_url\", StringType(), True),\n",
    "    StructField(\"u_refs\", StringType(), True),\n",
    "    StructField(\"p_refs\", StringType(), True),\n",
    "])\n",
    "\n",
    "base_path = \"./data/raw/reddit_test6\"\n",
    "\n",
    "def extract_references(comment):\n",
    "    user_references = re.findall(r'/u/\\w+', comment)\n",
    "    post_references = re.findall(r'/r/\\w+', comment)\n",
    "    return ','.join(user_references), ','.join(post_references)\n",
    "\n",
    "# Convert each RDD in the DStream to a DataFrame\n",
    "def process_rdd(time, rdd):\n",
    "    if not rdd.isEmpty():\n",
    "        extracted_data = rdd.map(lambda x: {**x, \"u_refs\": extract_references(x['link_url'])[0], \"p_refs\": extract_references(x['link_url'])[1]})\n",
    "        df = ss1.createDataFrame(extracted_data, schema)\n",
    "        \n",
    "        # Register the DataFrame as a temporary table named \"raw\"\n",
    "        df.createOrReplaceTempView(\"raw\")\n",
    "        \n",
    "        # Persist the DataFrame\n",
    "        df.persist()\n",
    "        \n",
    "        # Write the DataFrame to disk\n",
    "        output_path = f\"{base_path}/{time.strftime('%Y%m%d%H%M%S')}\"\n",
    "        df.write.json(output_path)\n",
    "        \n",
    "        # Show the DataFrame for debugging\n",
    "        df.show()\n",
    "\n",
    "        # Count the occurrences in the window\n",
    "        count = df.count()\n",
    "        print(f\"Number of occurrences in the window: {count}\")\n",
    "\n",
    "        # Save the count to a separate file\n",
    "        #count_path = f\"{base_path}_counts/{time.strftime('%Y%m%d%H%M%S')}_count.txt\"\n",
    "        #with open(count_path, 'w') as f:\n",
    "         #   f.write(str(count))\n",
    "\n",
    "comments.foreachRDD(process_rdd)\n",
    "\n",
    "# Start the streaming context\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab892c3-9f53-42fa-a56d-9b16b3d4cb33",
   "metadata": {},
   "source": [
    " ### Test Code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

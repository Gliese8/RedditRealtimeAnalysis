{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receive and Save Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part shows how to receive Reddit JSON content through a socket and save the raw data to disk using Spark DStreams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Create a SparkSession and StreamingContext\n",
    "spark_conf = SparkConf().setAppName(\"reddit\")\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "ssc = StreamingContext(spark.sparkContext, 5)  # 5 seconds batch interval\n",
    "\n",
    "# Create a DStream\n",
    "lines = ssc.socketTextStream(\"localhost\", 9998)\n",
    "comments = lines.map(lambda json_data: json.loads(json_data))\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"author\", StringType(), True),\n",
    "    StructField(\"subreddit\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Base path to save raw data\n",
    "base_path = \"./data/raw/reddit_sub\"\n",
    "\n",
    "# Convert each RDD in the DStream to a DataFrame\n",
    "def process_rdd(time, rdd):\n",
    "    if not rdd.isEmpty():\n",
    "        df = spark.createDataFrame(rdd, schema)\n",
    "        df.createOrReplaceTempView(\"comments\")\n",
    "        df.persist()\n",
    "        output_path = f\"{base_path}/{time.strftime('%Y%m%d%H%M%S')}\"\n",
    "        df.write.json(output_path)\n",
    "        df.show()\n",
    "\n",
    "comments.foreachRDD(process_rdd)\n",
    "\n",
    "# Start the streaming context\n",
    "ssc.start()\n",
    "# No ssc.awaitTermination() here to make the cell non-blocking and to use other cells in parallel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and Analyze Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part shows how to use Spark Structured Streaming to load data from disk, process it, and perform various transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create a SparkSession\n",
    "spark_conf = SparkConf().setAppName(\"reddit\")\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "\n",
    "# Path to input data\n",
    "input_path = \"./data/raw/reddit_v5/*/*.json\"\n",
    "\n",
    "# Define the schema for the input data\n",
    "schema = StructType([\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"author\", StringType(), True),\n",
    "    StructField(\"subreddit\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read the input data as a streaming DataFrame\n",
    "streaming_df = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(schema) \\\n",
    "    .option(\"path\", input_path) \\\n",
    "    .load()\n",
    "\n",
    "# Transform the data\n",
    "transformed_df = streaming_df \\\n",
    "    .withColumn('created_utc_ts', F.to_timestamp(F.col('date'))) \\\n",
    "    .withWatermark(\"created_utc_ts\", \"5 seconds\") \\\n",
    "    .groupBy(F.col(\"author\"), F.window(F.col(\"created_utc_ts\"), \"60 seconds\")) \\\n",
    "    .agg({\"created_utc_ts\": 'max', \"title\": 'count'}) \\\n",
    "    .select(F.col(\"author\"), F.col(\"window\"), F.col(\"max(created_utc_ts)\").alias(\"last_post_time\"), F.col(\"count(title)\").alias(\"post_count\"))\n",
    "\n",
    "# Save output to disk\n",
    "output_path = \"./data/processed/reddit_v1\"\n",
    "checkpt_path = \"./metadata/processed/reddit_v1\"\n",
    "\n",
    "transformed_df.writeStream \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"checkpointLocation\", checkpt_path) \\\n",
    "    .option(\"path\", output_path) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "# For testing, output to console (uncomment to use)\n",
    "# query = transformed_df.writeStream \\\n",
    "#     .format(\"console\") \\\n",
    "#     .start()\n",
    "\n",
    "# Mixing static table with historical content and dynamic table with content from current window\n",
    "base_path = \"./data/raw/reddit_v5/*/*.json\"\n",
    "historical = spark.read.json(base_path)\n",
    "historical.createOrReplaceTempView('historical')\n",
    "historical.show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM comments as ct\n",
    "    LEFT JOIN historical as ht on ht.author = ct.author\n",
    "    \"\"\").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import explode, split, col\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Define the host and port\n",
    "host = \"127.0.0.1\"\n",
    "port = 9998\n",
    "\n",
    "# Initialize SparkContext and SparkSession\n",
    "sc = SparkContext(\"local[2]\", \"DisplayLinesV2\")\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "ssc = StreamingContext(sc, 60)  # Set batch interval to 60 seconds\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "def process_rdd(rdd):\n",
    "    \"\"\"\n",
    "    Process each RDD and perform the following actions:\n",
    "    1. Convert RDD to DataFrame and persist it.\n",
    "    2. Extract references to users, posts, and URLs.\n",
    "    3. Calculate TF-IDF.\n",
    "    4. Find the top words and their TF-IDF scores.\n",
    "    5. Store metrics in a temporary table.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not rdd.isEmpty():\n",
    "        # Convert RDD to DataFrame and persist it.\n",
    "        records = rdd.map(lambda record: json.loads(record)).collect()\n",
    "        if records:\n",
    "            df = spark.createDataFrame(records)\n",
    "            df.createOrReplaceTempView(\"raw\")\n",
    "            df.write.json(\"/data/raw/reddit_ask\", mode=\"append\")\n",
    "            \n",
    "            # Extract references to users, posts, and URLs.\n",
    "            references = df.select(\n",
    "                explode(split(col(\"text\"), \" \")).alias(\"word\")\n",
    "            ).filter(\n",
    "                (col(\"word\").like(\"/u/%\")) |\n",
    "                (col(\"word\").like(\"/r/%\")) |\n",
    "                (col(\"word\").like(\"http%\"))\n",
    "            )\n",
    "            \n",
    "            # User references.\n",
    "            user_refs = references.filter(col(\"word\").like(\"/u/%\")).groupBy(\"word\").count()\n",
    "            user_refs.createOrReplaceTempView(\"user_references\")\n",
    "            user_refs.write.json(\"/path/to/output/user_references\", mode=\"append\")\n",
    "\n",
    "            # Post references.\n",
    "            post_refs = references.filter(col(\"word\").like(\"/r/%\")).groupBy(\"word\").count()\n",
    "            post_refs.createOrReplaceTempView(\"post_references\")\n",
    "            post_refs.write.json(\"/path/to/output/post_references\", mode=\"append\")\n",
    "            \n",
    "            # URL references.\n",
    "            url_refs = references.filter(col(\"word\").like(\"http%\")).groupBy(\"word\").count()\n",
    "            url_refs.createOrReplaceTempView(\"url_references\")\n",
    "            url_refs.write.json(\"/path/to/output/url_references\", mode=\"append\")\n",
    "            \n",
    "            # Calculate TF-IDF.\n",
    "            words_data = df.select(explode(split(col(\"text\"), \" \")).alias(\"words\"))\n",
    "            hashing_tf = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "            idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "            tf = hashing_tf.transform(words_data)\n",
    "            idf_model = idf.fit(tf)\n",
    "            tfidf_data = idf_model.transform(tf)\n",
    "            \n",
    "            # Find the top words and their TF-IDF scores.\n",
    "            top_words = tfidf_data.select(\"words\", \"features\").rdd.flatMap(\n",
    "                lambda row: [(row.words, float(v)) for v in row.features]\n",
    "            ).sortBy(lambda x: -x[1]).take(10)\n",
    "            \n",
    "            # Convert top words and their TF-IDF scores to DataFrame and persist it.\n",
    "            top_words_df = spark.createDataFrame(top_words, [\"word\", \"tfidf\"])\n",
    "            top_words_df.createOrReplaceTempView(\"top_words\")\n",
    "            top_words_df.write.json(\"/path/to/output/top_words\", mode=\"append\")\n",
    "            \n",
    "            # Store metrics in a temporary table.\n",
    "            user_refs.join(post_refs, user_refs.word == post_refs.word, \"outer\") \\\n",
    "\n",
    "print(\"Waiting for messages...\")\n",
    "lines = ssc.socketTextStream(host, port)\n",
    "\n",
    "# Apply processing function to each RDD\n",
    "lines.foreachRDD(lambda rdd: process_rdd(rdd))\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
